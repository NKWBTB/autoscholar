#+TITLE: Sentence alignment Study
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [10pt]
#+LATEX_HEADER: \usepackage{fullpage}
#+LATEX_HEADER: \usepackage{indentfirst}

* Algorithms and Open Source Implementations

This paper studies the problem of approximate string matching. In
particular, given a pattern string $p$ and a long text string $t$,
find a substring $t'$ of $t$ such that $t'$ has the smallest edit
distance from $p$.

** Bioinformation

Similar problem is extensively studie in bioinformation for alignment
of DNA sequences to find similar functions, known as /sequence
alignment/, with the Needleman–Wunsch algorithm
cite:1970-Journal-Needleman-General and the Smith–Waterman algorithm
cite:1981-Journal-Smith-Identification being the important ones.
There are many tools provided in this domain bioinformation [fn:ebi],
e.g. GeneWise cite:URL-GeneWise. However, these are typically
specialized for DNA sequence matching, and provides web interface
instead of versatile open source implementations.

[fn:ebi] https://www.ebi.ac.uk/Tools/psa/

** Computer Science
In computer science, the problem is known as approximate string
matching, or fuzzy string search. The mostly used algorithm is by
dynamic programming. The /Levenshtein distance/ can be computed
dynamically, and the problem of approximate string matching can be
computed exactly the same way.

There is another algorithm called /Bitap algorithm/, also known as
/shift-or/, /shift-and/ or /Baeza-Yates–Gonnet algorithm/, for
approximate string matching. The algorithm does most of the work with
bitwise operations, which are extremely fast. It is used in the
=agrep= Unix utility implemented with the TRE library [fn:tre].

Three implementations are generally available:

- /fuzzywuzzy/ [fn:fuzzywuzzy] is a fuzzy string matching in
  python. Under the hood, it uses
  /python-Levenshtein/ [fn:levenshtein]. The two projects are not well
  maintained and well-written, and I found the only function
  =partial_ratio= that is related to computing approximate string
  matching very suspicious [fn:fuzzywuzzy-issues].
- /fuzzysearch/ [fn:fuzzysearch] implements a "efficient ad-hoc
  searching" via linear programming. It accepts general sequences
  instead of merely strings, thus it can be used to support word-level
  distance with input of sequence of words.
- The TRE library [fn:tre] provides a very fast implementations for
  fuzzy string search, providing a C library, a utility tool =agrep=,
  and a python binding. This can only deal with string (instead of
  words), because it is basically an regular expression library. The
  utility command line tool =agrep= can also be used.

# #+BEGIN_EXAMPLE shell
# agrep
#  -E 10000 # number of allowed errors
#  -d "hfdsfjadsl" # an arbitrary value to use as 'line separator', It is '\n' by default
#   --show-position # show starting index of the match
#  "The approximate pattern string"
#  /path/to/file/to/match
# #+END_EXAMPLE

[fn:fuzzywuzzy] https://github.com/seatgeek/fuzzywuzzy
[fn:levenshtein] https://github.com/miohtama/python-Levenshtein
[fn:fuzzywuzzy-issues] https://github.com/seatgeek/fuzzywuzzy/issues/207
[fn:fuzzysearch] https://github.com/taleinat/fuzzysearch
[fn:tre] https://github.com/laurikari/tre

** Natural Language Processing
In NLP, the =monolingual-word-aligner= [fn:word-aligner-sultan] by
Sultan et al cite:2015-Workshop-Sultan-DLS exploits semantic and
contextual similarities of the words to make alignment decisions.
Ferrero extends it [fn:word-aligner-ferrero] adds a IDF weighting in
the Jaccard distance formula (approach presented by Brychcin
cite:2016-Workshop-Brychcin-UWB). The outputs are word-level
alignments, and not very related to the problem. For example, given
the input sentences:

#+BEGIN_EXAMPLE
sentence1 = "Four men died in an accident."
sentence2 = "4 people are dead from a collision."
#+END_EXAMPLE

It will output:
#+BEGIN_EXAMPLE
[[7, 8], [2, 2], [3, 4], [1, 1], [6, 7], [5, 6]]
[['men', 'people'], ['died', 'dead'], ['Four', '4'],
 ['accident', 'collision'], ['an', 'a']]
#+END_EXAMPLE


[fn:word-aligner-sultan] https://github.com/ma-sultan/monolingual-word-aligner
[fn:word-aligner-ferrero] https://github.com/FerreroJeremy/monolingual-word-aligner

* Experiment
Two aligners, /fuzzysearch/ and TRE library via python binding are
evaluated in this section. Tests are the html files generated from
PDF. The highlighted sentences are extracted, randomly mutated, and
used as pattern to search in the entire document. 4 files are tested
for evaluating the different settings, while all 40 files are tested
and all worked well.

Two settings are evaluated, namely different allowed edit distances
and mutation ratios. Distance is /Levenstein distance/, measured by
characters for /TRE/, and by words for /fuzzysearch/. Mutation ratio
measures how much the highlighted sentences are mutated, so =0.1=
means about =10%= of the words in the sentence are either added,
deleted, or substituted with random generated string.

Output metrics include time and precision. Time are reported in
seconds for all tests. =success= and =fail= show the number of
highlights whose identified begin and end index is within threshold 30
of the correct range. The shift is necessary because the mutation can
happen at the beginning and end of the pattern string.

Finally, short patterns are treated. The pattern that are less or
equal to 2 words are skipped. Mutation only happens for sentences that
is longer or equal to 10 words.

The results in the four tables show that /TRE/ scale well, while
/fuzzysearch/ is exponential. Both has very good precision, the few
failed cases, upon inspection, typically has a shift more than 30 but
less than 100, the sentence is still identified correctly. Mutation
ratio does not affect the effectiveness in the experimented settings.

#+CAPTION: TRE distance test (mutation ratio=0.1)
#+ATTR_LATEX: :font \small
| max distance | time(s) | success | fail |
|            / |       < |       < |    < |
|--------------+---------+---------+------|
|           10 |     8.7 |     120 |  133 |
|           30 |    17.0 |     232 |   21 |
|           50 |    24.5 |     247 |    6 |
|           80 |    31.0 |     251 |    2 |
|          100 |    32.8 |     253 |    0 |
|          150 |    35.1 |     251 |    2 |
|          200 |    36.3 |     253 |    0 |

#+CAPTION: fuzzysearch distance test (mutation ratio=0.1)
| max distance | time(s) | success | fail |
|            / |       < |       < |    < |
|--------------+---------+---------+------|
|            5 |     8.6 |     236 |   17 |
|            8 |    30.4 |     248 |    5 |
|           10 |   122.7 |     249 |    4 |
|           13 |   868.5 |     248 |    5 |

#+CAPTION: TRE mutation ratio test (distance=100)
| ratio | time | success | fail |
|     / |    < |       < |    < |
|-------+------+---------+------|
|  0.05 | 33.0 |     253 |    0 |
|   0.1 | 33.0 |     253 |    0 |
|   0.2 | 33.2 |     253 |    0 |
|   0.3 | 33.0 |     253 |    0 |

#+CAPTION: fuzzysearch mutation ratio test (distance=8)
| ratio | time | success | fail |
|     / |    < |       < |    < |
|-------+------+---------+------|
|  0.05 | 30.4 |     247 |    6 |
|   0.1 | 30.5 |     247 |    6 |
|   0.2 | 30.8 |     246 |    7 |
|   0.3 | 30.2 |     246 |    7 |


bibliography:../../../research/bib/manual/nlp.bib,../../../research/bib/manual/url.bib
bibliographystyle:plain
